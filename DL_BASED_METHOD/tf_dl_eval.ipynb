{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-28 16:44:16.717694: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-28 16:44:17.368770: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-28 16:44:17.369439: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-28 16:44:17.424821: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-08-28 16:44:17.424845: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ap30901\n",
      "2021-08-28 16:44:17.424850: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ap30901\n",
      "2021-08-28 16:44:17.424916: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.63.1\n",
      "2021-08-28 16:44:17.424933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.63.1\n",
      "2021-08-28 16:44:17.424937: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.63.1\n"
     ]
    }
   ],
   "source": [
    "#from numpy.random import seed\n",
    "#seed(42)\n",
    "#import tensorflow\n",
    "#tensorflow.random.set_seed(42)\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= \"-1\"\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess =tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_extraction import *\n",
    "from resp_signal_extraction import *\n",
    "from rr_extration import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "from tf_model import *\n",
    "from tf_model_evi import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "import evidential_deep_learning as edl\n",
    "import matplotlib.pyplot as plt\n",
    "from filters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_conf = 'conff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('output','rb') as f:\n",
    "    output_data = pkl.load(f)\n",
    "\n",
    "with open('input','rb') as f:\n",
    "    input_data = pkl.load(f)\n",
    "\n",
    "with open('raw_signal.pkl','rb') as f:\n",
    "    raw_data = pkl.load(f)\n",
    "\n",
    "input_data = np.transpose(input_data, (0,2,1))\n",
    "raw_data = np.transpose(raw_data, (0,2,1))\n",
    "annotation = pd.read_pickle('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/annotation.pkl')\n",
    "reference_rr = (annotation['Reference_RR'].values).reshape(-1,1)\n",
    "\n",
    "input_data = np.around(input_data , decimals = 4)\n",
    "raw_data = np.around(raw_data , decimals = 4)\n",
    "output_data = np.around(output_data , decimals = 4)\n",
    "reference_rr = np.around(reference_rr , decimals = 4)\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input_data, dtype = 'float32')\n",
    "tensor_output = tf.convert_to_tensor(output_data, dtype = 'float32')\n",
    "tensor_ref_rr = tf.convert_to_tensor(reference_rr, dtype = 'float32')\n",
    "tensor_raw_data = tf.convert_to_tensor(raw_data, dtype = 'float32')\n",
    "training_ids = annotation['patient_id'] < 13\n",
    "\n",
    "x_train_data = tensor_input[tf.convert_to_tensor(training_ids.values)]\n",
    "x_test_data = tensor_input[tf.convert_to_tensor(~(training_ids.values))]\n",
    "x_train_ref_rr = tensor_ref_rr[tf.convert_to_tensor(training_ids.values)]\n",
    "x_test_ref_rr = tensor_ref_rr[tf.convert_to_tensor(~(training_ids.values))]\n",
    "x_train_raw_sig = tensor_raw_data[tf.convert_to_tensor(training_ids.values)]\n",
    "x_test_raw_sig = tensor_raw_data[tf.convert_to_tensor(~(training_ids.values))]\n",
    "\n",
    "y_train_data = tensor_output[tf.convert_to_tensor(training_ids.values)]\n",
    "y_test_data = tensor_output[tf.convert_to_tensor(~(training_ids.values))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confc':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_data , y_train_data))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_data)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_data , y_test_data))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "\n",
    "if input_conf == 'confb':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_data , x_train_ref_rr))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_data)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_data , x_test_ref_rr))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "\n",
    "if input_conf == 'confd':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_data , y_train_data, x_train_ref_rr))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_data)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_data , y_test_data, x_test_ref_rr))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "if input_conf == 'confa':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw_sig , x_train_ref_rr))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_raw_sig)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw_sig , x_test_ref_rr))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "\n",
    "if input_conf == 'confe':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw_sig , y_train_data))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_raw_sig)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw_sig , y_test_data))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "if input_conf == 'conff':\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw_sig , y_train_data, x_train_ref_rr))\n",
    "        train_dataset = train_dataset.shuffle(len(x_train_data)).batch(128)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw_sig , y_test_data, x_test_ref_rr))\n",
    "        test_dataset = test_dataset.batch(128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confc':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/confc/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step,(x_batch_test , y_batch_test) in enumerate(test_dataset):\n",
    "        output = model(x_batch_test)\n",
    "        test_loss = loss_fn(y_batch_test,output)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confd':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet_Multi_resp(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/confd/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    final_output_rr = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test,y_batch_test,x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        test_output,test_out_rr = model(x_batch_test)\n",
    "        test_loss_resp = loss_fn(y_batch_test  , test_output)\n",
    "        test_loss_rr = loss_fn(x_batch_test_ref_rr , test_out_rr)\n",
    "        test_loss = test_loss_resp + test_loss_rr\n",
    "        if step == 0:\n",
    "            final_output = test_output\n",
    "            final_output_rr = test_out_rr\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , test_output] , axis = 0)\n",
    "            final_output_rr = tf.concat([final_output_rr , test_out_rr] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "            \n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confb':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet_Encoder(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/confb/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test,x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        output = model(x_batch_test)\n",
    "        test_loss = loss_fn(x_batch_test_ref_rr , output)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confe':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/confe/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw,y_batch_test) in enumerate(test_dataset):\n",
    "        output = model(x_batch_test_raw)\n",
    "        test_loss = loss_fn(y_batch_test , output)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confa':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw_encoder(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/confa/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw, x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        output = model(x_batch_test_raw)\n",
    "        test_loss = loss_fn(x_batch_test_ref_rr , output)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'conff':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw_multi(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODELS/conff/best_model_100.h5')        \n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    final_output_rr = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw , y_batch_test , x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        test_output,test_out_rr = model(x_batch_test_raw)\n",
    "        test_loss_resp = loss_fn(y_batch_test  , test_output)\n",
    "        test_loss_rr = loss_fn(x_batch_test_ref_rr , test_out_rr)\n",
    "        test_loss = test_loss_resp + test_loss_rr\n",
    "        if step == 0:\n",
    "            final_output = test_output\n",
    "            final_output_rr = test_out_rr\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , test_output] , axis = 0)\n",
    "            final_output_rr = tf.concat([final_output_rr , test_out_rr] , axis = 0)\n",
    "        test_loss_list.append(test_loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extremas_extraction(signal):\n",
    "    avg_breath_duration = np.array([])\n",
    "    extrema_relevent = []\n",
    "    for item in signal:\n",
    "        amplitude = np.array([])\n",
    "        pos_peaks , _ = scipy.signal.find_peaks(item , height = [-3000,3000])\n",
    "        neg_peaks , _ = scipy.signal.find_peaks(-1*item , height = [-3000 , 3000])\n",
    "        extremas = np.concatenate((pos_peaks , neg_peaks))\n",
    "        extremas = np.sort(extremas)\n",
    "        for i in range(len(extremas)):\n",
    "            amplitude = np.append(amplitude , item[int(extremas[i])])\n",
    "        amplitude_diff = np.abs(np.diff(amplitude))\n",
    "        q3 = np.percentile(amplitude_diff , 75)\n",
    "        threshold = 0.3*q3\n",
    "        eliminate_pairs_of_extrema = 1\n",
    "        while(eliminate_pairs_of_extrema):\n",
    "            amps = np.array([])\n",
    "            if len(extremas)<3:\n",
    "                eliminate_pairs_of_extrema = 0\n",
    "                continue\n",
    "            for i in range(len(extremas)):\n",
    "                amps = np.append(amps , item[int(extremas[i])])\n",
    "            amp_diff = np.abs(np.diff(amps)) \n",
    "            min_amp_diff , index = min(amp_diff) , np.argmin(amp_diff)\n",
    "            #print(min_amp_diff)\n",
    "            if min_amp_diff > threshold:\n",
    "                eliminate_pairs_of_extrema = 0\n",
    "                #extrema_relevent = extremas\n",
    "            else:\n",
    "                extremas = np.concatenate((extremas[0:index] , extremas[index+2 :]))\n",
    "                #amplitude_diff = np.delete(amplitude_diff , index)\n",
    "        if item[int(extremas[0])] < item[int(extremas[1])]:\n",
    "            extremas = extremas[1:]\n",
    "        if item[int(extremas[-1])] < item[int(extremas[-2])]:\n",
    "            extremas = extremas[:-1]\n",
    "        no_of_breaths = (len(extremas)-1)/2\n",
    "        breath_duration = extremas[-1] - extremas[0]\n",
    "        avg_breath_duration = np.append(avg_breath_duration , breath_duration/no_of_breaths)\n",
    "        extrema_relevent.append(extremas)\n",
    "    return avg_breath_duration , extrema_relevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sig = y_test_data.numpy()\n",
    "fbpB , fbpA = band_pass(0.1,0.8,8)\n",
    "final_ref_resp_sig = []\n",
    "for item in ref_sig:\n",
    "    final_ref_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "final_ref_resp_sig = np.array(final_ref_resp_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confa':\n",
    "    final_output_rr = final_output.numpy()\n",
    "    final_output_rr = final_output_rr.reshape(final_output_rr.shape[0],final_output_rr.shape[1])\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    error = np.abs(avg_ref_breath - final_output_rr)\n",
    "    mae = np.mean(error)\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    print('Mean Absolute Error for {} is: {}'.format(input_conf,mae))\n",
    "    print('Root Mean Square Error for {} is: {}'.format(input_conf,rmse))\n",
    "#----------------------------------------------------------------------------------------------------------------   \n",
    "if input_conf == 'confb':\n",
    "    final_output_rr = final_output.numpy()\n",
    "    final_output_rr = final_output_rr.reshape(final_output_rr.shape[0],final_output_rr.shape[1])\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    error = np.abs(avg_ref_breath - final_output_rr)\n",
    "    mae = np.mean(error)\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    print('Mean Absolute Error for {} is: {}'.format(input_conf,mae))\n",
    "    print('Root Mean Square Error for {} is: {}'.format(input_conf,rmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confc':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    \n",
    "    final_output_resp = final_output.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0],final_output_resp.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_breaths = (60*4/duration_resp).reshape(-1,1)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - avg_breaths)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "\n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------    \n",
    "if input_conf == 'confe':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    final_output_resp = final_output.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0],final_output_resp.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_breaths = (60*4/duration_resp).reshape(-1,1)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - avg_breaths)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error average wise for conff is: 2.4993221593655948\n",
      "Root Mean Square Error average wise for conff is: 3.141458412459503\n",
      "Mean Absolute Error instantaneous wise for conff is: 3.3477551310206475\n",
      "Root Mean Square Error instantaneous wise for conff is: 4.1437087364766425\n"
     ]
    }
   ],
   "source": [
    "if input_conf == 'confd':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    \n",
    "    final_output_resp = final_output.numpy()\n",
    "    final_rr = final_output_rr.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0],final_output_resp.shape[1])\n",
    "    final_rr = final_rr.reshape(final_rr.shape[0],final_rr.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "        \n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - final_rr)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'conff':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    \n",
    "    final_output_resp = final_output.numpy()\n",
    "    final_rr = final_output_rr.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0],final_output_resp.shape[1])\n",
    "    final_rr = final_rr.reshape(final_rr.shape[0],final_rr.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "        \n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - final_rr)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH EVIDENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confc':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet_evi(model_input_shape)\n",
    "    coeff_val = 0.01\n",
    "    #loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/confc/best_model__coeff_0.01.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step,(x_batch_test , y_batch_test) in enumerate(test_dataset):\n",
    "        y_batch_test = tf.expand_dims(y_batch_test , axis = -1)\n",
    "        output = model(x_batch_test)\n",
    "        test_loss = edl.losses.EvidentialRegression(y_batch_test,output, coeff = coeff_val)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confe':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw_evi(model_input_shape)\n",
    "    coeff_val = 0.01\n",
    "    #loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/confe/best_model___coeff_0.01.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw,y_batch_test) in enumerate(test_dataset):\n",
    "        y_batch_test = tf.expand_dims(y_batch_test , axis = -1)\n",
    "        output = model(x_batch_test_raw)\n",
    "        test_loss = edl.losses.EvidentialRegression(y_batch_test,output, coeff = coeff_val)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confb':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet_Encoder_evi(model_input_shape)\n",
    "    coeff_val = 0.005\n",
    "    #loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/confb/best_model___coeff_0.005.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test,x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        x_batch_test_ref_rr = tf.expand_dims(x_batch_test_ref_rr , axis = -1)\n",
    "        output = model(x_batch_test)\n",
    "        test_loss = edl.losses.EvidentialRegression(x_batch_test_ref_rr , output , coeff = coeff_val)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confa':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw_encoder_evi(model_input_shape)\n",
    "    #loss_fn = Huber()\n",
    "    coeff_val = 0.01\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/confa/best_model___coeff_0.01.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw, x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        x_batch_test_ref_rr = tf.expand_dims(x_batch_test_ref_rr , axis = -1)\n",
    "        output = model(x_batch_test_raw)\n",
    "        test_loss = edl.losses.EvidentialRegression(x_batch_test_ref_rr , output, coeff = coeff_val)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)  \n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'conff':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw_multi_evi(model_input_shape)\n",
    "    #loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    coeff_val = 0.05\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/conff/best_model___coeff_0.05.h5')        \n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    final_output_rr = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw , y_batch_test , x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        y_batch_test = tf.expand_dims(y_batch_test , axis = -1)\n",
    "        x_batch_test_ref_rr = tf.expand_dims(x_batch_test_ref_rr , axis = -1)\n",
    "        test_output,test_out_rr = model(x_batch_test_raw)\n",
    "        test_loss_resp = edl.losses.EvidentialRegression(y_batch_test  , test_output , coeff = coeff_val)\n",
    "        test_loss_rr = edl.losses.EvidentialRegression(x_batch_test_ref_rr , test_out_rr , coeff = coeff_val)\n",
    "        test_loss = test_loss_resp + test_loss_rr\n",
    "        if step == 0:\n",
    "            final_output = test_output\n",
    "            final_output_rr = test_out_rr\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , test_output] , axis = 0)\n",
    "            final_output_rr = tf.concat([final_output_rr , test_out_rr] , axis = 0)\n",
    "        test_loss_list.append(test_loss) \n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confd':\n",
    "    model_input_shape = (128,3)\n",
    "    model  = BRUnet_Multi_resp_evi(model_input_shape)\n",
    "    #loss_fn = Huber()\n",
    "    model(tf.ones((128,128,3)))\n",
    "    coeff_val = 0.005\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/SAVED_MODEL_WITH_EVI/confd/best_model___coeff_0.005.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    final_output_rr = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test,y_batch_test,x_batch_test_ref_rr) in enumerate(test_dataset):\n",
    "        x_batch_test_ref_rr = tf.expand_dims(x_batch_test_ref_rr , axis = -1)\n",
    "        y_batch_test = tf.expand_dims(y_batch_test , axis = -1)\n",
    "        test_output,test_out_rr = model(x_batch_test)\n",
    "        test_loss_resp = edl.losses.EvidentialRegression(y_batch_test  , test_output, coeff = coeff_val)\n",
    "        test_loss_rr = edl.losses.EvidentialRegression(x_batch_test_ref_rr , test_out_rr,coeff = coeff_val)\n",
    "        test_loss = test_loss_resp + test_loss_rr\n",
    "        if step == 0:\n",
    "            final_output = test_output\n",
    "            final_output_rr = test_out_rr\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , test_output] , axis = 0)\n",
    "            final_output_rr = tf.concat([final_output_rr , test_out_rr] , axis = 0)\n",
    "        test_loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, lamda, alpha, beta = tf.split(final_output, 4, axis=-1)\n",
    "output = gamma.numpy()\n",
    "output = output.reshape(output.shape[0] , output.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error average wise for conff is: 2.833721939665406\n",
      "Root Mean Square Error average wise for conff is: 3.628248891912498\n",
      "Mean Absolute Error instantaneous wise for conff is: 2.8323736439980878\n",
      "Root Mean Square Error instantaneous wise for conff is: 3.5479606903605947\n",
      "[[ 23.912018]\n",
      " [ 23.911526]\n",
      " [ 25.463623]\n",
      " [ 23.91217 ]\n",
      " [ 23.912079]\n",
      " [ 23.912157]\n",
      " [ 23.91209 ]\n",
      " [ 23.9122  ]\n",
      " [ 23.912104]\n",
      " [ 23.91209 ]\n",
      " [ 23.91212 ]\n",
      " [ 23.91213 ]\n",
      " [ 23.912079]\n",
      " [ 23.912134]\n",
      " [ 23.912134]\n",
      " [ 23.912128]\n",
      " [ 23.912178]\n",
      " [ 23.91217 ]\n",
      " [ 23.91219 ]\n",
      " [ 23.912113]\n",
      " [ 23.912138]\n",
      " [ 23.911911]\n",
      " [ 24.53528 ]\n",
      " [ 23.91184 ]\n",
      " [ 23.911087]\n",
      " [ 23.911394]\n",
      " [ 23.911343]\n",
      " [ 23.90998 ]\n",
      " [ 23.911022]\n",
      " [ 23.910774]\n",
      " [ 23.773735]\n",
      " [ 24.062284]\n",
      " [ 23.903204]\n",
      " [ 23.911713]\n",
      " [ 23.89345 ]\n",
      " [ 23.950438]\n",
      " [ 24.340145]\n",
      " [ 23.90118 ]\n",
      " [ 23.946722]\n",
      " [ 24.334238]\n",
      " [ 23.910378]\n",
      " [ 23.911638]\n",
      " [ 23.91222 ]\n",
      " [ 23.911606]\n",
      " [ 23.91219 ]\n",
      " [ 23.912302]\n",
      " [ 23.91198 ]\n",
      " [ 24.309637]\n",
      " [ 23.909353]\n",
      " [ 23.910097]\n",
      " [ 23.911713]\n",
      " [ 23.913069]\n",
      " [ 23.911215]\n",
      " [ 23.911802]\n",
      " [ 23.911886]\n",
      " [ 23.911629]\n",
      " [ 23.911066]\n",
      " [ 23.911465]\n",
      " [ 23.911999]\n",
      " [ 23.911148]\n",
      " [ 23.911049]\n",
      " [ 23.909523]\n",
      " [ 23.91177 ]\n",
      " [ 23.911976]\n",
      " [ 23.904644]\n",
      " [ 23.911154]\n",
      " [ 23.910892]\n",
      " [ 23.910585]\n",
      " [ 23.911863]\n",
      " [ 23.912056]\n",
      " [ 23.911654]\n",
      " [ 23.909101]\n",
      " [ 23.911154]\n",
      " [ 23.91772 ]\n",
      " [ 23.911451]\n",
      " [ 23.907658]\n",
      " [ 23.911488]\n",
      " [ 27.527044]\n",
      " [ 23.910166]\n",
      " [ 23.910465]\n",
      " [ 23.909626]\n",
      " [ 23.899939]\n",
      " [ 23.911146]\n",
      " [ 23.911001]\n",
      " [ 23.912748]\n",
      " [ 23.91189 ]\n",
      " [ 23.910973]\n",
      " [ 24.951387]\n",
      " [ 23.911745]\n",
      " [ 23.906656]\n",
      " [ 23.90941 ]\n",
      " [ 23.911654]\n",
      " [ 39.565052]\n",
      " [ 23.922167]\n",
      " [ 24.577261]\n",
      " [ 23.910727]\n",
      " [ 23.909908]\n",
      " [ 24.237299]\n",
      " [ 24.183372]\n",
      " [ 23.911556]\n",
      " [ 23.91151 ]\n",
      " [ 24.783394]\n",
      " [ 23.905853]\n",
      " [ 23.91186 ]\n",
      " [ 23.912344]\n",
      " [ 23.910711]\n",
      " [ 23.91606 ]\n",
      " [ 23.911854]\n",
      " [ 37.099564]\n",
      " [ 24.802189]\n",
      " [ 23.911673]\n",
      " [ 32.65886 ]\n",
      " [ 27.955887]\n",
      " [ 23.90785 ]\n",
      " [ 23.910973]\n",
      " [ 23.91169 ]\n",
      " [ 23.911194]\n",
      " [ 25.103172]\n",
      " [ 23.909868]\n",
      " [ 23.909636]\n",
      " [ 25.275526]\n",
      " [ 25.27262 ]\n",
      " [ 23.912048]\n",
      " [ 24.641075]\n",
      " [ 23.911886]\n",
      " [ 27.598495]\n",
      " [ 23.911985]\n",
      " [ 23.911037]\n",
      " [ 23.89937 ]\n",
      " [ 23.912294]\n",
      " [ 23.897232]\n",
      " [ 25.174807]\n",
      " [ 23.912504]\n",
      " [ 23.912643]\n",
      " [ 24.43309 ]\n",
      " [ 23.909695]\n",
      " [ 23.911585]\n",
      " [ 23.911274]\n",
      " [ 23.910269]\n",
      " [ 23.910585]\n",
      " [ 23.90997 ]\n",
      " [ 25.492086]\n",
      " [ 23.90638 ]\n",
      " [ 23.91051 ]\n",
      " [ 23.902678]\n",
      " [ 23.911888]\n",
      " [ 23.912056]\n",
      " [ 23.911785]\n",
      " [ 23.900074]\n",
      " [ 23.91219 ]\n",
      " [ 23.911814]\n",
      " [ 23.912327]\n",
      " [ 23.911423]\n",
      " [ 23.91169 ]\n",
      " [ 23.911945]\n",
      " [ 23.911783]\n",
      " [ 23.911146]\n",
      " [ 23.912186]\n",
      " [ 23.912004]\n",
      " [ 23.91198 ]\n",
      " [ 23.912113]\n",
      " [ 23.912064]\n",
      " [ 23.911999]\n",
      " [ 23.911875]\n",
      " [ 23.912079]\n",
      " [ 23.912277]\n",
      " [ 23.912027]\n",
      " [ 23.911966]\n",
      " [ 23.911854]\n",
      " [ 23.912104]\n",
      " [ 23.911934]\n",
      " [ 23.912085]\n",
      " [ 23.912138]\n",
      " [ 23.912064]\n",
      " [ 23.911922]\n",
      " [ 23.911705]\n",
      " [ 23.91166 ]\n",
      " [ 23.911861]\n",
      " [ 23.912037]\n",
      " [ 23.911898]\n",
      " [ 23.912018]\n",
      " [ 23.912212]\n",
      " [ 23.912367]\n",
      " [ 23.911764]\n",
      " [ 23.911951]\n",
      " [ 23.912443]\n",
      " [ 23.912048]\n",
      " [ 23.911886]\n",
      " [ 23.91189 ]\n",
      " [ 23.911743]\n",
      " [ 23.911629]\n",
      " [ 23.91177 ]\n",
      " [ 23.91209 ]\n",
      " [ 23.91204 ]\n",
      " [ 23.911966]\n",
      " [ 23.912077]\n",
      " [ 23.911829]\n",
      " [ 23.91186 ]\n",
      " [ 23.911764]\n",
      " [ 23.911648]\n",
      " [ 23.91186 ]\n",
      " [ 23.911676]\n",
      " [ 23.91198 ]\n",
      " [ 23.911394]\n",
      " [ 23.911743]\n",
      " [ 23.910873]\n",
      " [ 23.909908]\n",
      " [ 23.911995]\n",
      " [ 23.911951]\n",
      " [ 23.911629]\n",
      " [ 23.911543]\n",
      " [ 23.910873]\n",
      " [ 23.910559]\n",
      " [ 23.911875]\n",
      " [ 23.911266]\n",
      " [ 23.909023]\n",
      " [ 23.909393]\n",
      " [ 25.988998]\n",
      " [ 23.909794]\n",
      " [ 24.306046]\n",
      " [ 23.90799 ]\n",
      " [ 23.911394]\n",
      " [ 23.910936]\n",
      " [ 23.909473]\n",
      " [ 26.094181]\n",
      " [ 23.909855]\n",
      " [ 23.911068]\n",
      " [ 23.90871 ]\n",
      " [ 23.911346]\n",
      " [ 23.911432]\n",
      " [ 23.910807]\n",
      " [ 23.909729]\n",
      " [ 23.911266]\n",
      " [ 23.910858]\n",
      " [ 23.911945]\n",
      " [ 23.912064]\n",
      " [ 23.911291]\n",
      " [ 23.912157]\n",
      " [ 23.91209 ]\n",
      " [ 23.912218]\n",
      " [ 23.912004]\n",
      " [ 23.911936]\n",
      " [ 23.912264]\n",
      " [ 23.912178]\n",
      " [ 23.912157]\n",
      " [ 23.91215 ]\n",
      " [ 23.91219 ]\n",
      " [ 23.9122  ]\n",
      " [ 23.912178]\n",
      " [ 23.91217 ]\n",
      " [ 23.912155]\n",
      " [ 23.91213 ]\n",
      " [ 23.91219 ]\n",
      " [ 23.912134]\n",
      " [ 23.912218]\n",
      " [ 23.91215 ]\n",
      " [ 23.912212]\n",
      " [ 23.91438 ]\n",
      " [ 27.004145]\n",
      " [ 23.909399]\n",
      " [ 23.906916]\n",
      " [ 23.910177]\n",
      " [ 23.909523]\n",
      " [ 23.909874]\n",
      " [ 23.909348]\n",
      " [ 23.906218]\n",
      " [ 23.905272]\n",
      " [ 23.903172]\n",
      " [ 23.931664]\n",
      " [ 23.910349]\n",
      " [ 23.911758]\n",
      " [ 23.911432]\n",
      " [ 23.911238]\n",
      " [ 23.912056]\n",
      " [ 23.912033]\n",
      " [ 23.911764]\n",
      " [ 23.894295]\n",
      " [ 23.908892]\n",
      " [ 23.912155]\n",
      " [ 23.911343]\n",
      " [ 23.91079 ]\n",
      " [ 23.91222 ]\n",
      " [ 23.912128]\n",
      " [ 23.91215 ]\n",
      " [ 23.912104]\n",
      " [ 23.911839]\n",
      " [ 26.94485 ]\n",
      " [ 23.852337]\n",
      " [ 23.911985]\n",
      " [ 23.911932]\n",
      " [ 23.910185]\n",
      " [ 23.91021 ]\n",
      " [ 23.909184]\n",
      " [ 23.907509]\n",
      " [ 23.910612]\n",
      " [ 23.90625 ]\n",
      " [ 23.909052]\n",
      " [ 23.907862]\n",
      " [ 23.905   ]\n",
      " [ 23.909494]\n",
      " [ 23.909908]\n",
      " [ 27.235905]\n",
      " [ 26.461866]\n",
      " [ 23.907341]\n",
      " [ 23.910767]\n",
      " [ 23.910858]\n",
      " [ 23.903795]\n",
      " [ 23.912085]\n",
      " [ 26.039236]\n",
      " [ 34.15837 ]\n",
      " [ 38.799732]\n",
      " [ 29.447851]\n",
      " [ 32.38639 ]\n",
      " [ 31.041056]\n",
      " [ 43.56535 ]\n",
      " [ 33.11548 ]\n",
      " [ 31.623247]\n",
      " [ 43.404766]\n",
      " [ 38.260456]\n",
      " [ 29.651787]\n",
      " [ 35.95176 ]\n",
      " [ 40.895607]\n",
      " [ 36.754723]\n",
      " [ 52.70028 ]\n",
      " [ 33.06655 ]\n",
      " [ 46.565853]\n",
      " [ 60.731613]\n",
      " [ 24.793602]\n",
      " [ 26.50906 ]\n",
      " [ 31.899307]\n",
      " [ 32.049026]\n",
      " [ 30.304506]\n",
      " [ 28.345543]\n",
      " [ 30.697437]\n",
      " [ 35.25742 ]\n",
      " [ 26.866774]\n",
      " [ 28.502253]\n",
      " [ 28.036303]\n",
      " [ 24.931341]\n",
      " [ 31.491838]\n",
      " [ 29.749062]\n",
      " [ 29.010551]\n",
      " [ 22.64611 ]\n",
      " [ 36.102123]\n",
      " [ 42.210632]\n",
      " [ 29.250784]\n",
      " [ 39.86854 ]\n",
      " [ 40.26998 ]\n",
      " [ 36.16473 ]\n",
      " [ 30.231607]\n",
      " [ 22.995665]\n",
      " [ 31.849371]\n",
      " [ 22.895523]\n",
      " [ 65.9286  ]\n",
      " [ 25.915468]\n",
      " [ 44.237305]\n",
      " [ 28.7903  ]\n",
      " [ 25.308565]\n",
      " [ 29.066269]\n",
      " [ 41.32837 ]\n",
      " [ 32.333626]\n",
      " [ 48.19899 ]\n",
      " [ 37.35335 ]\n",
      " [ 36.50377 ]\n",
      " [ 28.576288]\n",
      " [ 44.841373]\n",
      " [ 41.59946 ]\n",
      " [ 36.40544 ]\n",
      " [ 50.021904]\n",
      " [ 48.26829 ]\n",
      " [ 25.991177]\n",
      " [ 26.637257]\n",
      " [ 36.685413]\n",
      " [ 29.239054]\n",
      " [ 33.194614]\n",
      " [ 26.853964]\n",
      " [ 33.20765 ]\n",
      " [ 44.26659 ]\n",
      " [ 33.061172]\n",
      " [ 43.459106]\n",
      " [ 50.836735]\n",
      " [ 35.185875]\n",
      " [ 37.27412 ]\n",
      " [ 32.40797 ]\n",
      " [ 45.693962]\n",
      " [ 27.51549 ]\n",
      " [ 26.001764]\n",
      " [ 31.200842]\n",
      " [ 29.885593]\n",
      " [ 30.892292]\n",
      " [ 32.576687]\n",
      " [ 31.669865]\n",
      " [ 39.257065]\n",
      " [ 79.422005]\n",
      " [ 33.451977]\n",
      " [ 37.027195]\n",
      " [ 53.89867 ]\n",
      " [ 72.459015]\n",
      " [ 56.971485]\n",
      " [ 50.713512]\n",
      " [ 49.443798]\n",
      " [ 42.03502 ]\n",
      " [ 34.46746 ]\n",
      " [ 74.14759 ]\n",
      " [ 50.820045]\n",
      " [ 53.75671 ]\n",
      " [ 45.184513]\n",
      " [ 35.603287]\n",
      " [ 42.539074]\n",
      " [ 35.826202]\n",
      " [ 27.59999 ]\n",
      " [ 36.988094]\n",
      " [ 29.83513 ]\n",
      " [ 30.753263]\n",
      " [ 33.37754 ]\n",
      " [ 44.840313]\n",
      " [ 32.333855]\n",
      " [ 28.498568]\n",
      " [ 40.278454]\n",
      " [ 30.728811]\n",
      " [ 29.681398]\n",
      " [ 30.726465]\n",
      " [ 30.075554]\n",
      " [ 30.5388  ]\n",
      " [ 46.628746]\n",
      " [ 37.825424]\n",
      " [ 48.724396]\n",
      " [ 50.538364]\n",
      " [ 32.676697]\n",
      " [ 29.279827]\n",
      " [ 50.140205]\n",
      " [ 45.40397 ]\n",
      " [ 39.70939 ]\n",
      " [ 58.267147]\n",
      " [ 30.687296]\n",
      " [ 32.00778 ]\n",
      " [ 47.078384]\n",
      " [ 27.826414]\n",
      " [ 29.470924]\n",
      " [ 33.05356 ]\n",
      " [ 27.681055]\n",
      " [ 27.09493 ]\n",
      " [ 34.030624]\n",
      " [ 48.67208 ]\n",
      " [ 24.20269 ]\n",
      " [ 28.224924]\n",
      " [ 22.968336]\n",
      " [ 23.9065  ]\n",
      " [ 23.907364]\n",
      " [ 23.913792]\n",
      " [ 24.636793]\n",
      " [ 22.929363]\n",
      " [ 25.827114]\n",
      " [ 23.91369 ]\n",
      " [ 23.91797 ]\n",
      " [ 29.149378]\n",
      " [ 26.258118]\n",
      " [ 23.90965 ]\n",
      " [ 23.911472]\n",
      " [ 23.910612]\n",
      " [ 23.911394]\n",
      " [ 23.91235 ]\n",
      " [ 28.449764]\n",
      " [ 25.226912]\n",
      " [ 23.278286]\n",
      " [ 23.908508]\n",
      " [ 28.223316]\n",
      " [ 23.911713]\n",
      " [ 25.2871  ]\n",
      " [ 23.238613]\n",
      " [ 23.911499]\n",
      " [ 25.806608]\n",
      " [ 23.906965]\n",
      " [ 23.911106]\n",
      " [ 23.911234]\n",
      " [ 25.211933]\n",
      " [ 26.774921]\n",
      " [ 23.910666]\n",
      " [ 23.911936]\n",
      " [ 23.911442]\n",
      " [ 23.888643]\n",
      " [ 23.908478]\n",
      " [ 23.907333]\n",
      " [ 23.687782]\n",
      " [ 23.91209 ]\n",
      " [ 34.21377 ]\n",
      " [ 28.34515 ]\n",
      " [ 46.767933]\n",
      " [ 56.64459 ]\n",
      " [ 34.38774 ]\n",
      " [ 52.79342 ]\n",
      " [ 32.235275]\n",
      " [ 80.06201 ]\n",
      " [125.95397 ]\n",
      " [ 69.9358  ]\n",
      " [ 71.984245]\n",
      " [ 87.117775]\n",
      " [ 96.22583 ]\n",
      " [ 80.1465  ]\n",
      " [ 39.80907 ]\n",
      " [ 29.522093]\n",
      " [ 60.52563 ]\n",
      " [ 46.358273]\n",
      " [ 41.99076 ]\n",
      " [ 66.19111 ]\n",
      " [ 29.881039]\n",
      " [ 65.99096 ]\n",
      " [ 72.67556 ]\n",
      " [ 32.233837]\n",
      " [ 34.841877]\n",
      " [ 23.914312]\n",
      " [ 24.655668]\n",
      " [ 44.927437]\n",
      " [ 32.046974]\n",
      " [ 34.008564]\n",
      " [ 31.247915]\n",
      " [ 25.712646]\n",
      " [ 23.592512]\n",
      " [ 23.557804]\n",
      " [ 23.36852 ]\n",
      " [ 30.109999]\n",
      " [ 46.183838]\n",
      " [ 24.415277]\n",
      " [ 30.334925]\n",
      " [ 26.212215]\n",
      " [ 23.912079]\n",
      " [ 23.911718]\n",
      " [ 22.344877]\n",
      " [ 26.664894]\n",
      " [ 28.292013]\n",
      " [ 24.612598]\n",
      " [ 26.134335]\n",
      " [ 24.413433]\n",
      " [ 32.700848]\n",
      " [ 33.074036]\n",
      " [ 30.479223]\n",
      " [ 29.237753]\n",
      " [ 25.226278]\n",
      " [ 23.26379 ]\n",
      " [ 23.542978]\n",
      " [ 30.270685]\n",
      " [ 31.91912 ]\n",
      " [ 23.372505]\n",
      " [ 27.568256]\n",
      " [ 24.959915]\n",
      " [ 24.255613]\n",
      " [ 23.295258]\n",
      " [ 46.32976 ]\n",
      " [ 35.61638 ]\n",
      " [ 41.76651 ]\n",
      " [ 64.21996 ]\n",
      " [ 44.233307]\n",
      " [ 42.344128]\n",
      " [103.12893 ]\n",
      " [ 50.902473]\n",
      " [ 26.585838]\n",
      " [ 26.355936]\n",
      " [ 40.386234]\n",
      " [ 47.11373 ]\n",
      " [ 36.64133 ]\n",
      " [ 26.630306]\n",
      " [ 38.612076]\n",
      " [ 23.900564]\n",
      " [ 23.910719]\n",
      " [ 23.90882 ]\n",
      " [ 23.898792]\n",
      " [ 30.09676 ]\n",
      " [ 38.157562]\n",
      " [ 25.67721 ]\n",
      " [ 29.62902 ]\n",
      " [ 28.796507]\n",
      " [ 24.941895]\n",
      " [ 26.566355]\n",
      " [ 28.87959 ]\n",
      " [ 27.263689]\n",
      " [ 27.22065 ]\n",
      " [ 28.64395 ]\n",
      " [ 26.014917]\n",
      " [ 28.248323]\n",
      " [ 29.688347]\n",
      " [ 27.328527]\n",
      " [ 28.83365 ]\n",
      " [ 28.40374 ]\n",
      " [ 31.128757]\n",
      " [ 28.669897]\n",
      " [ 25.81324 ]\n",
      " [ 26.45051 ]\n",
      " [ 32.82694 ]\n",
      " [ 58.372894]\n",
      " [ 53.176525]\n",
      " [ 48.33754 ]\n",
      " [ 63.954212]\n",
      " [ 61.265347]\n",
      " [ 66.31583 ]\n",
      " [ 39.1042  ]\n",
      " [ 59.7444  ]\n",
      " [ 66.06988 ]\n",
      " [ 59.64783 ]\n",
      " [ 61.142986]\n",
      " [ 58.805614]\n",
      " [ 59.86857 ]\n",
      " [ 57.062378]\n",
      " [ 73.88246 ]\n",
      " [ 91.10204 ]\n",
      " [ 33.469788]\n",
      " [ 50.034504]\n",
      " [ 28.437595]\n",
      " [ 49.639122]\n",
      " [ 51.003998]\n",
      " [ 43.642666]\n",
      " [ 40.307888]\n",
      " [ 42.895885]\n",
      " [ 73.23907 ]\n",
      " [ 38.43728 ]\n",
      " [ 31.629665]\n",
      " [ 31.90772 ]\n",
      " [ 34.849094]\n",
      " [ 47.021435]\n",
      " [ 46.662952]\n",
      " [ 25.813795]\n",
      " [ 55.40511 ]\n",
      " [ 55.254215]\n",
      " [ 60.68094 ]\n",
      " [ 62.12669 ]\n",
      " [ 32.830368]\n",
      " [ 46.837692]\n",
      " [ 23.489998]\n",
      " [ 32.526978]\n",
      " [ 34.602356]\n",
      " [ 40.332336]\n",
      " [ 36.8308  ]\n",
      " [ 27.429344]\n",
      " [ 27.67893 ]\n",
      " [ 34.164116]\n",
      " [ 52.79033 ]\n",
      " [ 38.899437]\n",
      " [ 48.846123]\n",
      " [ 62.400814]\n",
      " [ 37.148476]\n",
      " [ 44.20763 ]\n",
      " [ 57.041492]\n",
      " [ 40.585396]\n",
      " [ 66.78044 ]\n",
      " [ 64.79606 ]\n",
      " [ 59.912697]\n",
      " [ 29.499592]\n",
      " [ 60.782074]\n",
      " [ 44.47633 ]\n",
      " [ 54.27591 ]\n",
      " [ 56.301746]\n",
      " [ 57.8472  ]\n",
      " [ 65.00643 ]\n",
      " [ 73.79036 ]\n",
      " [ 58.624573]\n",
      " [ 51.1387  ]\n",
      " [ 40.152924]\n",
      " [ 33.735287]\n",
      " [ 30.963991]\n",
      " [ 28.733835]\n",
      " [ 30.524628]\n",
      " [ 29.287664]\n",
      " [ 56.740692]\n",
      " [ 80.9216  ]\n",
      " [ 35.566624]\n",
      " [ 45.597576]\n",
      " [ 42.25669 ]\n",
      " [ 23.197596]\n",
      " [ 48.195812]\n",
      " [ 41.717293]\n",
      " [ 37.506363]\n",
      " [ 65.64213 ]\n",
      " [ 79.26386 ]\n",
      " [ 56.221752]\n",
      " [ 56.01383 ]\n",
      " [ 28.660324]\n",
      " [ 31.6057  ]\n",
      " [ 51.25438 ]\n",
      " [ 37.576103]\n",
      " [ 30.919367]\n",
      " [ 33.309498]\n",
      " [ 24.521606]\n",
      " [ 27.607407]\n",
      " [ 36.884853]\n",
      " [ 36.484177]\n",
      " [ 42.27553 ]\n",
      " [ 29.23037 ]\n",
      " [ 39.69629 ]\n",
      " [ 41.45714 ]\n",
      " [ 43.904083]\n",
      " [ 70.34328 ]\n",
      " [ 43.62013 ]\n",
      " [ 55.3545  ]\n",
      " [ 64.188446]\n",
      " [ 50.537   ]\n",
      " [ 34.8076  ]\n",
      " [ 51.47662 ]\n",
      " [ 33.40968 ]\n",
      " [ 26.68125 ]\n",
      " [ 57.184048]\n",
      " [ 30.06995 ]\n",
      " [ 31.196142]\n",
      " [ 44.87749 ]\n",
      " [ 27.056465]\n",
      " [ 41.13055 ]\n",
      " [ 43.13796 ]\n",
      " [ 26.838972]\n",
      " [ 30.352396]\n",
      " [ 31.604645]\n",
      " [ 30.074799]\n",
      " [ 28.660877]\n",
      " [ 28.34525 ]\n",
      " [ 28.530855]\n",
      " [ 28.961597]\n",
      " [ 24.290194]\n",
      " [ 30.524092]\n",
      " [ 28.25956 ]\n",
      " [ 31.106937]\n",
      " [ 29.827545]\n",
      " [ 26.269032]\n",
      " [ 29.778944]\n",
      " [ 28.106125]\n",
      " [ 30.078259]\n",
      " [ 28.29484 ]\n",
      " [ 28.644695]\n",
      " [ 32.028168]\n",
      " [ 31.590334]\n",
      " [ 31.074072]\n",
      " [ 26.741026]\n",
      " [ 31.673632]\n",
      " [ 29.62008 ]\n",
      " [ 33.0139  ]\n",
      " [ 32.609818]\n",
      " [ 29.309683]\n",
      " [ 25.539642]\n",
      " [ 27.880882]\n",
      " [ 30.03786 ]\n",
      " [ 28.906933]\n",
      " [ 30.84236 ]\n",
      " [ 21.888327]\n",
      " [ 48.848648]\n",
      " [ 52.969177]\n",
      " [ 55.658955]\n",
      " [ 51.232826]\n",
      " [ 52.465458]\n",
      " [ 48.12962 ]\n",
      " [ 86.910965]\n",
      " [ 50.32619 ]\n",
      " [ 89.1141  ]\n",
      " [ 93.273636]\n",
      " [ 44.96539 ]\n",
      " [ 77.97302 ]\n",
      " [ 69.8172  ]\n",
      " [ 71.15386 ]\n",
      " [ 40.331123]\n",
      " [ 50.81262 ]\n",
      " [ 44.826523]\n",
      " [ 42.48896 ]\n",
      " [119.58073 ]\n",
      " [ 31.59313 ]\n",
      " [ 58.74893 ]\n",
      " [ 45.579372]\n",
      " [ 59.15895 ]\n",
      " [ 57.961372]\n",
      " [ 47.5428  ]\n",
      " [ 75.83079 ]\n",
      " [ 47.35121 ]\n",
      " [ 34.764114]\n",
      " [ 35.85063 ]\n",
      " [164.66759 ]\n",
      " [ 42.91381 ]\n",
      " [ 33.68131 ]\n",
      " [ 30.144804]\n",
      " [ 45.49307 ]\n",
      " [ 57.33278 ]\n",
      " [ 30.420961]\n",
      " [ 25.971537]\n",
      " [ 43.894604]\n",
      " [ 38.41618 ]\n",
      " [ 25.112041]\n",
      " [ 24.675999]\n",
      " [ 27.388746]\n",
      " [ 35.618927]\n",
      " [ 28.390612]\n",
      " [ 24.226889]\n",
      " [ 27.280512]\n",
      " [ 28.011332]\n",
      " [ 27.822035]\n",
      " [ 23.565313]\n",
      " [ 28.252296]\n",
      " [ 39.1396  ]\n",
      " [ 41.128613]\n",
      " [ 32.05571 ]\n",
      " [ 37.05909 ]\n",
      " [ 33.66865 ]\n",
      " [ 25.779844]\n",
      " [ 32.789196]\n",
      " [ 30.099361]\n",
      " [ 38.045605]\n",
      " [ 28.550783]\n",
      " [ 25.762884]\n",
      " [ 29.017363]\n",
      " [ 25.536827]\n",
      " [ 26.868809]\n",
      " [ 29.52491 ]\n",
      " [ 30.7605  ]\n",
      " [ 30.379381]\n",
      " [ 30.444702]\n",
      " [ 29.585766]\n",
      " [ 31.622435]\n",
      " [ 30.994604]\n",
      " [ 25.151016]\n",
      " [ 29.14343 ]\n",
      " [ 28.062637]\n",
      " [ 27.456984]]\n"
     ]
    }
   ],
   "source": [
    "if input_conf == 'confc':\n",
    "    final_output_resp_sig = []\n",
    "    gamma, lamda, alpha, beta = tf.split(final_output, 4, axis=-1)\n",
    "    output = gamma.numpy()\n",
    "    output = output.reshape(output.shape[0] , output.shape[1])\n",
    "    for item in output:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_breaths = (60*4/duration_resp).reshape(-1,1)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - avg_breaths)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    final_lamda = lamda.numpy()\n",
    "    final_alpha = alpha.numpy()\n",
    "    final_beta = beta.numpy()\n",
    "    \n",
    "    final_lamda = final_lamda.reshape(final_lamda.shape[0],final_lamda.shape[1])\n",
    "    final_alpha = final_alpha.reshape(final_alpha.shape[0],final_alpha.shape[1])\n",
    "    final_beta = final_beta.reshape(final_beta.shape[0],final_beta.shape[1])\n",
    "    #x_samps = np.arange(0,len(y_train_data[0]))\n",
    "    aleatoric = final_beta/(final_alpha - 1)\n",
    "    epistemic = final_beta/(final_lamda*(final_alpha - 1))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confe':\n",
    "    final_output_resp_sig = []\n",
    "    gamma, lamda, alpha, beta = tf.split(final_output, 4, axis=-1)\n",
    "    output = gamma.numpy()\n",
    "    output = output.reshape(output.shape[0] , output.shape[1])\n",
    "    for item in output:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_breaths = (60*4/duration_resp).reshape(-1,1)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - avg_breaths)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    final_lamda = lamda.numpy()\n",
    "    final_alpha = alpha.numpy()\n",
    "    final_beta = beta.numpy()\n",
    "    \n",
    "    final_lamda = final_lamda.reshape(final_lamda.shape[0],final_lamda.shape[1])\n",
    "    final_alpha = final_alpha.reshape(final_alpha.shape[0],final_alpha.shape[1])\n",
    "    final_beta = final_beta.reshape(final_beta.shape[0],final_beta.shape[1])\n",
    "    x_samps = np.arange(0,len(y_train_data[0]))\n",
    "    aleatoric = final_beta/(final_alpha - 1)\n",
    "    epistemic = final_beta/(final_lamda*(final_alpha - 1))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confb':\n",
    "    gamma, lamda, alpha, beta = tf.split(final_output, 4, axis=-1)\n",
    "    final_output_rr = gamma.numpy()\n",
    "    final_output_rr = final_output_rr.reshape(final_output_rr.shape[0],final_output_rr.shape[1])\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    error = np.abs(avg_ref_breath - final_output_rr)\n",
    "    mae = np.mean(error)\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    print('Mean Absolute Error for {} is: {}'.format(input_conf,mae))\n",
    "    print('Root Mean Square Error for {} is: {}'.format(input_conf,rmse))\n",
    "    \n",
    "    final_lamda = lamda.numpy()\n",
    "    final_alpha = alpha.numpy()\n",
    "    final_beta = beta.numpy()\n",
    "    \n",
    "    final_lamda = final_lamda.reshape(final_lamda.shape[0],final_lamda.shape[1])\n",
    "    final_alpha = final_alpha.reshape(final_alpha.shape[0],final_alpha.shape[1])\n",
    "    final_beta = final_beta.reshape(final_beta.shape[0],final_beta.shape[1])\n",
    "    #x_samps = np.arange(0,len(y_train_data[0]))\n",
    "    aleatoric = final_beta/(final_alpha - 1)\n",
    "    epistemic = final_beta/(final_lamda*(final_alpha - 1))\n",
    "    var = np.sqrt(epistemic)\n",
    "    print(epistemic)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confa':\n",
    "    gamma, lamda, alpha, beta = tf.split(final_output, 4, axis=-1)\n",
    "    final_output_rr = gamma.numpy()\n",
    "    final_output_rr = final_output_rr.reshape(final_output_rr.shape[0],final_output_rr.shape[1])\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    error = np.abs(avg_ref_breath - final_output_rr)\n",
    "    mae = np.mean(error)\n",
    "    rmse = np.sqrt(np.mean(error**2))\n",
    "    print('Mean Absolute Error for {} is: {}'.format(input_conf,mae))\n",
    "    print('Root Mean Square Error for {} is: {}'.format(input_conf,rmse))\n",
    "    \n",
    "    final_lamda = lamda.numpy()\n",
    "    final_alpha = alpha.numpy()\n",
    "    final_beta = beta.numpy()\n",
    "    \n",
    "    final_lamda = final_lamda.reshape(final_lamda.shape[0],final_lamda.shape[1])\n",
    "    final_alpha = final_alpha.reshape(final_alpha.shape[0],final_alpha.shape[1])\n",
    "    final_beta = final_beta.reshape(final_beta.shape[0],final_beta.shape[1])\n",
    "    aleatoric = final_beta/(final_alpha - 1)\n",
    "    epistemic = final_beta/(final_lamda*(final_alpha - 1))\n",
    "    print(epistemic)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'conff':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    \n",
    "    gamma_resp, lamda_resp, alpha_resp, beta_resp = tf.split(final_output, 4, axis=-1)\n",
    "    gamma_rr, lamda_rr, alpha_rr, beta_rr = tf.split(final_output_rr, 4, axis=-1)\n",
    "    final_output_resp = gamma_resp.numpy()\n",
    "    final_rr = gamma_rr.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0] , final_output_resp.shape[1])\n",
    "    final_rr = final_rr.reshape(final_rr.shape[0],final_rr.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    \n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - final_rr)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "    \n",
    "    final_lamda_resp = lamda_resp.numpy()\n",
    "    final_alpha_resp = alpha_resp.numpy()\n",
    "    final_beta_resp = beta_resp.numpy()\n",
    "    \n",
    "    final_lamda_rr = lamda_rr.numpy()\n",
    "    final_alpha_rr = alpha_rr.numpy()\n",
    "    final_beta_rr = beta_rr.numpy()\n",
    "    \n",
    "    aleatoric_resp = final_beta_resp/(final_alpha_resp - 1)\n",
    "    epistemic_resp = final_beta_resp/(final_lamda_resp*(final_alpha_resp - 1))\n",
    "    \n",
    "    aleatoric_rr = final_beta_rr/(final_alpha_rr - 1)\n",
    "    epistemic_rr = final_beta_rr/(final_lamda_rr*(final_alpha_rr - 1))\n",
    "    \n",
    "    epistemic_rr = epistemic_rr.reshape(epistemic_rr.shape[0],epistemic_rr.shape[1])\n",
    "    aleatoric_rr = aleatoric_rr.reshape(aleatoric_rr.shape[0],aleatoric_rr.shape[1])\n",
    "    print(epistemic_rr)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "if input_conf == 'confd':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    \n",
    "    gamma_resp, lamda_resp, alpha_resp, beta_resp = tf.split(final_output, 4, axis=-1)\n",
    "    gamma_rr, lamda_rr, alpha_rr, beta_rr = tf.split(final_output_rr, 4, axis=-1)\n",
    "    final_output_resp = gamma_resp.numpy()\n",
    "    final_rr = gamma_rr.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0] , final_output_resp.shape[1])\n",
    "    final_rr = final_rr.reshape(final_rr.shape[0],final_rr.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    \n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - final_rr)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "    \n",
    "    final_lamda_resp = lamda_resp.numpy()\n",
    "    final_alpha_resp = alpha_resp.numpy()\n",
    "    final_beta_resp = beta_resp.numpy()\n",
    "    \n",
    "    final_lamda_rr = lamda_rr.numpy()\n",
    "    final_alpha_rr = alpha_rr.numpy()\n",
    "    final_beta_rr = beta_rr.numpy()\n",
    "    \n",
    "    aleatoric_resp = final_beta_resp/(final_alpha_resp - 1)\n",
    "    epistemic_resp = final_beta_resp/(final_lamda_resp*(final_alpha_resp - 1))\n",
    "    \n",
    "    aleatoric_rr = final_beta_rr/(final_alpha_rr - 1)\n",
    "    epistemic_rr = final_beta_rr/(final_lamda_rr*(final_alpha_rr - 1))\n",
    "    epistemic_rr = epistemic_rr.reshape(epistemic_rr.shape[0],epistemic_rr.shape[1])\n",
    "    aleatoric_rr = aleatoric_rr.reshape(aleatoric_rr.shape[0],aleatoric_rr.shape[1])\n",
    "    print(epistemic_rr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confe':\n",
    "    model_input_shape = (2048,3)\n",
    "    model  = BRUnet_raw(model_input_shape)\n",
    "    loss_fn = Huber()\n",
    "    model(tf.ones((128,2048,3)))\n",
    "    model.load_weights('/media/acrophase/Sentinel_1/charan/BR_Uncertainty/DL_BASED_METHOD/TEST_SAVE_MODEL/confe/best_model_100.h5')\n",
    "    test_loss_list = []\n",
    "    final_output = tf.convert_to_tensor([])\n",
    "    for step , (x_batch_test_raw,y_batch_test) in enumerate(test_dataset):\n",
    "        output = model(x_batch_test_raw)\n",
    "        test_loss = loss_fn(y_batch_test , output)\n",
    "        if step == 0:\n",
    "            final_output = output\n",
    "        else:\n",
    "            final_output = tf.concat([final_output , output] , axis = 0)\n",
    "        test_loss_list.append(test_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_conf == 'confe':\n",
    "    final_output_resp_sig = []\n",
    "    inst_br_dur = []\n",
    "    inst_ref_br_dur = []\n",
    "    inst_rr = []\n",
    "    inst_ref_rr = []\n",
    "    final_output_resp = final_output.numpy()\n",
    "    final_output_resp = final_output_resp.reshape(final_output_resp.shape[0],final_output_resp.shape[1])\n",
    "    for item in final_output_resp:\n",
    "        final_output_resp_sig.append(scipy.signal.filtfilt(fbpB,fbpA , item))\n",
    "    final_output_resp_sig = np.array(final_output_resp_sig)\n",
    "    duration_resp,extremas_resp = extremas_extraction(final_output_resp_sig)\n",
    "    duration_ref_resp,extremas_ref_resp = extremas_extraction(final_ref_resp_sig)\n",
    "    avg_breaths = (60*4/duration_resp).reshape(-1,1)\n",
    "    avg_ref_breath = (60*4/duration_ref_resp).reshape(-1,1)\n",
    "    \n",
    "    error_avg_breaths = np.abs(avg_ref_breath - avg_breaths)\n",
    "    mae_avg_breath = np.mean(error_avg_breaths)\n",
    "    rmse_avg_breath = np.sqrt(np.mean(error_avg_breaths**2))\n",
    "    \n",
    "    for item in extremas_resp:\n",
    "        inst_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in extremas_ref_resp:\n",
    "        inst_ref_br_dur.append(np.diff(item[0::2]))\n",
    "    for item in inst_br_dur:\n",
    "        inst_rr.append(np.mean(60*4/item))\n",
    "    for item in inst_ref_br_dur:\n",
    "        inst_ref_rr.append(np.mean(60*4/item))\n",
    "    \n",
    "    inst_rr = (np.array(inst_rr)).reshape(-1,1)\n",
    "    inst_ref_rr = (np.array(inst_ref_rr)).reshape(-1,1)\n",
    "    \n",
    "    error_inst_breaths = np.abs(inst_ref_rr - inst_rr)\n",
    "    mae_inst_breath = np.mean(error_inst_breaths)\n",
    "    rmse_inst_breath = np.sqrt(np.mean(error_inst_breaths**2))\n",
    "    \n",
    "    print('Mean Absolute Error average wise for {} is: {}'.format(input_conf,mae_avg_breath))\n",
    "    print('Root Mean Square Error average wise for {} is: {}'.format(input_conf,rmse_avg_breath))\n",
    "    \n",
    "    print('Mean Absolute Error instantaneous wise for {} is: {}'.format(input_conf,mae_inst_breath))\n",
    "    print('Root Mean Square Error instantaneous wise for {} is: {}'.format(input_conf,rmse_inst_breath))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1428430533.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_121178/1428430533.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Mean Absolute Error average wise for confe is: 2.4133602018065368\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Mean Absolute Error average wise for confe is: 2.4133602018065368\n",
    "Root Mean Square Error average wise for confe is: 3.05533378095587\n",
    "Mean Absolute Error instantaneous wise for confe is: 2.5210396786558924\n",
    "Root Mean Square Error instantaneous wise for confe is: 3.13225322961194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
